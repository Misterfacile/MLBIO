{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-10T12:12:49.968645Z",
     "iopub.status.busy": "2024-10-10T12:12:49.968210Z",
     "iopub.status.idle": "2024-10-10T12:12:54.047609Z",
     "shell.execute_reply": "2024-10-10T12:12:54.046216Z",
     "shell.execute_reply.started": "2024-10-10T12:12:49.968600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T12:13:10.781753Z",
     "iopub.status.busy": "2024-10-10T12:13:10.781022Z",
     "iopub.status.idle": "2024-10-10T12:13:19.390343Z",
     "shell.execute_reply": "2024-10-10T12:13:19.389162Z",
     "shell.execute_reply.started": "2024-10-10T12:13:10.781709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 12663522.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 311667.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 732646.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2011459.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:18:51.908721Z",
     "iopub.status.busy": "2024-10-10T13:18:51.908259Z",
     "iopub.status.idle": "2024-10-10T13:18:51.974469Z",
     "shell.execute_reply": "2024-10-10T13:18:51.973381Z",
     "shell.execute_reply.started": "2024-10-10T13:18:51.908677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "indices = list(range(len(train_dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "subset1_indices = indices[:600]\n",
    "subset2_indices = indices[600:1200]\n",
    "\n",
    "subset1 = Subset(train_dataset, subset1_indices)\n",
    "subset2 = Subset(train_dataset, subset2_indices)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader1 = DataLoader(subset1, batch_size=batch_size, shuffle=True)\n",
    "train_loader2 = DataLoader(subset2, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:19:08.895494Z",
     "iopub.status.busy": "2024-10-10T13:19:08.894644Z",
     "iopub.status.idle": "2024-10-10T13:19:08.920638Z",
     "shell.execute_reply": "2024-10-10T13:19:08.919491Z",
     "shell.execute_reply.started": "2024-10-10T13:19:08.895446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc1 = nn.Linear(64*4*4, 512)  \n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model1 = SimpleCNN()\n",
    "model2 = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:19:09.244226Z",
     "iopub.status.busy": "2024-10-10T13:19:09.243756Z",
     "iopub.status.idle": "2024-10-10T13:19:09.251672Z",
     "shell.execute_reply": "2024-10-10T13:19:09.250487Z",
     "shell.execute_reply.started": "2024-10-10T13:19:09.244181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def average_model_parameters(models, average_weight):\n",
    "    avg_model = SimpleCNN()\n",
    "    model_params = [list(model.parameters()) for model in models]\n",
    "    \n",
    "    avg_model_params = list(avg_model.parameters())\n",
    "    \n",
    "    for param_idx in range(len(model_params[0])):\n",
    "        avg_param = sum([average_weight[i] * model_params[i][param_idx].data for i in range(len(models))])\n",
    "        avg_model_params[param_idx].data.copy_(avg_param)\n",
    "        \n",
    "    return avg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:19:09.503246Z",
     "iopub.status.busy": "2024-10-10T13:19:09.502146Z",
     "iopub.status.idle": "2024-10-10T13:19:09.509361Z",
     "shell.execute_reply": "2024-10-10T13:19:09.508059Z",
     "shell.execute_reply.started": "2024-10-10T13:19:09.503189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def update_model_parameters(model: keras.Model, new_weights: list):\n",
    "    if len(new_weights) != len(model.get_weights()):\n",
    "        raise ValueError(\"The number of new weights must match the number of layers in the model.\")\n",
    "\n",
    "    model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:19:09.750370Z",
     "iopub.status.busy": "2024-10-10T13:19:09.749943Z",
     "iopub.status.idle": "2024-10-10T13:19:09.755921Z",
     "shell.execute_reply": "2024-10-10T13:19:09.754667Z",
     "shell.execute_reply.started": "2024-10-10T13:19:09.750329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def update_model_parameters(model, params):\n",
    "    for param, new_param in zip(model.parameters(), params):\n",
    "        param.data.copy_(new_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:28:25.570994Z",
     "iopub.status.busy": "2024-10-10T13:28:25.570526Z",
     "iopub.status.idle": "2024-10-10T13:30:26.210784Z",
     "shell.execute_reply": "2024-10-10T13:30:26.209567Z",
     "shell.execute_reply.started": "2024-10-10T13:28:25.570950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [10/12], Loss: 0.0183\n",
      "Epoch [1], Batch [10/12], Loss: 0.0299\n",
      "Round 1 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0193\n",
      "Epoch [1], Batch [10/12], Loss: 0.0317\n",
      "Round 2 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0191\n",
      "Epoch [1], Batch [10/12], Loss: 0.0318\n",
      "Round 3 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0192\n",
      "Epoch [1], Batch [10/12], Loss: 0.0311\n",
      "Round 4 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0193\n",
      "Epoch [1], Batch [10/12], Loss: 0.0260\n",
      "Round 5 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0206\n",
      "Epoch [1], Batch [10/12], Loss: 0.0298\n",
      "Round 6 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0195\n",
      "Epoch [1], Batch [10/12], Loss: 0.0323\n",
      "Round 7 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0176\n",
      "Epoch [1], Batch [10/12], Loss: 0.0315\n",
      "Round 8 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0165\n",
      "Epoch [1], Batch [10/12], Loss: 0.0272\n",
      "Round 9 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0181\n",
      "Epoch [1], Batch [10/12], Loss: 0.0286\n",
      "Round 10 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0181\n",
      "Epoch [1], Batch [10/12], Loss: 0.0269\n",
      "Round 11 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0182\n",
      "Epoch [1], Batch [10/12], Loss: 0.0281\n",
      "Round 12 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0180\n",
      "Epoch [1], Batch [10/12], Loss: 0.0209\n",
      "Round 13 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0184\n",
      "Epoch [1], Batch [10/12], Loss: 0.0288\n",
      "Round 14 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0178\n",
      "Epoch [1], Batch [10/12], Loss: 0.0281\n",
      "Round 15 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0174\n",
      "Epoch [1], Batch [10/12], Loss: 0.0266\n",
      "Round 16 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0160\n",
      "Epoch [1], Batch [10/12], Loss: 0.0254\n",
      "Round 17 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0143\n",
      "Epoch [1], Batch [10/12], Loss: 0.0229\n",
      "Round 18 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0172\n",
      "Epoch [1], Batch [10/12], Loss: 0.0247\n",
      "Round 19 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0165\n",
      "Epoch [1], Batch [10/12], Loss: 0.0224\n",
      "Round 20 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0162\n",
      "Epoch [1], Batch [10/12], Loss: 0.0230\n",
      "Round 21 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0148\n",
      "Epoch [1], Batch [10/12], Loss: 0.0198\n",
      "Round 22 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0169\n",
      "Epoch [1], Batch [10/12], Loss: 0.0212\n",
      "Round 23 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0148\n",
      "Epoch [1], Batch [10/12], Loss: 0.0193\n",
      "Round 24 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0163\n",
      "Epoch [1], Batch [10/12], Loss: 0.0195\n",
      "Round 25 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0153\n",
      "Epoch [1], Batch [10/12], Loss: 0.0208\n",
      "Round 26 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0154\n",
      "Epoch [1], Batch [10/12], Loss: 0.0232\n",
      "Round 27 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0150\n",
      "Epoch [1], Batch [10/12], Loss: 0.0190\n",
      "Round 28 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0145\n",
      "Epoch [1], Batch [10/12], Loss: 0.0229\n",
      "Round 29 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0149\n",
      "Epoch [1], Batch [10/12], Loss: 0.0160\n",
      "Round 30 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0152\n",
      "Epoch [1], Batch [10/12], Loss: 0.0211\n",
      "Round 31 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0132\n",
      "Epoch [1], Batch [10/12], Loss: 0.0218\n",
      "Round 32 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0146\n",
      "Epoch [1], Batch [10/12], Loss: 0.0225\n",
      "Round 33 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0132\n",
      "Epoch [1], Batch [10/12], Loss: 0.0202\n",
      "Round 34 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0146\n",
      "Epoch [1], Batch [10/12], Loss: 0.0207\n",
      "Round 35 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0136\n",
      "Epoch [1], Batch [10/12], Loss: 0.0170\n",
      "Round 36 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0132\n",
      "Epoch [1], Batch [10/12], Loss: 0.0201\n",
      "Round 37 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0131\n",
      "Epoch [1], Batch [10/12], Loss: 0.0184\n",
      "Round 38 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0136\n",
      "Epoch [1], Batch [10/12], Loss: 0.0207\n",
      "Round 39 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0130\n",
      "Epoch [1], Batch [10/12], Loss: 0.0203\n",
      "Round 40 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0130\n",
      "Epoch [1], Batch [10/12], Loss: 0.0196\n",
      "Round 41 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0123\n",
      "Epoch [1], Batch [10/12], Loss: 0.0173\n",
      "Round 42 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0124\n",
      "Epoch [1], Batch [10/12], Loss: 0.0184\n",
      "Round 43 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0131\n",
      "Epoch [1], Batch [10/12], Loss: 0.0138\n",
      "Round 44 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0123\n",
      "Epoch [1], Batch [10/12], Loss: 0.0175\n",
      "Round 45 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0130\n",
      "Epoch [1], Batch [10/12], Loss: 0.0155\n",
      "Round 46 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0120\n",
      "Epoch [1], Batch [10/12], Loss: 0.0181\n",
      "Round 47 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0119\n",
      "Epoch [1], Batch [10/12], Loss: 0.0166\n",
      "Round 48 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0120\n",
      "Epoch [1], Batch [10/12], Loss: 0.0182\n",
      "Round 49 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0115\n",
      "Epoch [1], Batch [10/12], Loss: 0.0146\n",
      "Round 50 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0113\n",
      "Epoch [1], Batch [10/12], Loss: 0.0134\n",
      "Round 51 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0112\n",
      "Epoch [1], Batch [10/12], Loss: 0.0162\n",
      "Round 52 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0117\n",
      "Epoch [1], Batch [10/12], Loss: 0.0142\n",
      "Round 53 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0107\n",
      "Epoch [1], Batch [10/12], Loss: 0.0172\n",
      "Round 54 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0117\n",
      "Epoch [1], Batch [10/12], Loss: 0.0153\n",
      "Round 55 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0110\n",
      "Epoch [1], Batch [10/12], Loss: 0.0170\n",
      "Round 56 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0121\n",
      "Epoch [1], Batch [10/12], Loss: 0.0161\n",
      "Round 57 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0111\n",
      "Epoch [1], Batch [10/12], Loss: 0.0142\n",
      "Round 58 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0101\n",
      "Epoch [1], Batch [10/12], Loss: 0.0147\n",
      "Round 59 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0109\n",
      "Epoch [1], Batch [10/12], Loss: 0.0162\n",
      "Round 60 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0109\n",
      "Epoch [1], Batch [10/12], Loss: 0.0123\n",
      "Round 61 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0109\n",
      "Epoch [1], Batch [10/12], Loss: 0.0134\n",
      "Round 62 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0101\n",
      "Epoch [1], Batch [10/12], Loss: 0.0126\n",
      "Round 63 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0099\n",
      "Epoch [1], Batch [10/12], Loss: 0.0149\n",
      "Round 64 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0099\n",
      "Epoch [1], Batch [10/12], Loss: 0.0144\n",
      "Round 65 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0097\n",
      "Epoch [1], Batch [10/12], Loss: 0.0137\n",
      "Round 66 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0101\n",
      "Epoch [1], Batch [10/12], Loss: 0.0141\n",
      "Round 67 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0097\n",
      "Epoch [1], Batch [10/12], Loss: 0.0129\n",
      "Round 68 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0093\n",
      "Epoch [1], Batch [10/12], Loss: 0.0141\n",
      "Round 69 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0098\n",
      "Epoch [1], Batch [10/12], Loss: 0.0142\n",
      "Round 70 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0104\n",
      "Epoch [1], Batch [10/12], Loss: 0.0136\n",
      "Round 71 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0085\n",
      "Epoch [1], Batch [10/12], Loss: 0.0151\n",
      "Round 72 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0089\n",
      "Epoch [1], Batch [10/12], Loss: 0.0128\n",
      "Round 73 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0095\n",
      "Epoch [1], Batch [10/12], Loss: 0.0120\n",
      "Round 74 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0099\n",
      "Epoch [1], Batch [10/12], Loss: 0.0132\n",
      "Round 75 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0099\n",
      "Epoch [1], Batch [10/12], Loss: 0.0120\n",
      "Round 76 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0090\n",
      "Epoch [1], Batch [10/12], Loss: 0.0128\n",
      "Round 77 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0091\n",
      "Epoch [1], Batch [10/12], Loss: 0.0123\n",
      "Round 78 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0095\n",
      "Epoch [1], Batch [10/12], Loss: 0.0113\n",
      "Round 79 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0089\n",
      "Epoch [1], Batch [10/12], Loss: 0.0127\n",
      "Round 80 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0089\n",
      "Epoch [1], Batch [10/12], Loss: 0.0122\n",
      "Round 81 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0089\n",
      "Epoch [1], Batch [10/12], Loss: 0.0119\n",
      "Round 82 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0084\n",
      "Epoch [1], Batch [10/12], Loss: 0.0111\n",
      "Round 83 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0087\n",
      "Epoch [1], Batch [10/12], Loss: 0.0117\n",
      "Round 84 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0094\n",
      "Epoch [1], Batch [10/12], Loss: 0.0103\n",
      "Round 85 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0087\n",
      "Epoch [1], Batch [10/12], Loss: 0.0114\n",
      "Round 86 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0088\n",
      "Epoch [1], Batch [10/12], Loss: 0.0110\n",
      "Round 87 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0081\n",
      "Epoch [1], Batch [10/12], Loss: 0.0122\n",
      "Round 88 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0091\n",
      "Epoch [1], Batch [10/12], Loss: 0.0112\n",
      "Round 89 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0087\n",
      "Epoch [1], Batch [10/12], Loss: 0.0099\n",
      "Round 90 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0083\n",
      "Epoch [1], Batch [10/12], Loss: 0.0106\n",
      "Round 91 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0082\n",
      "Epoch [1], Batch [10/12], Loss: 0.0093\n",
      "Round 92 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0083\n",
      "Epoch [1], Batch [10/12], Loss: 0.0105\n",
      "Round 93 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0082\n",
      "Epoch [1], Batch [10/12], Loss: 0.0105\n",
      "Round 94 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0088\n",
      "Epoch [1], Batch [10/12], Loss: 0.0103\n",
      "Round 95 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0085\n",
      "Epoch [1], Batch [10/12], Loss: 0.0085\n",
      "Round 96 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0074\n",
      "Epoch [1], Batch [10/12], Loss: 0.0104\n",
      "Round 97 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0073\n",
      "Epoch [1], Batch [10/12], Loss: 0.0099\n",
      "Round 98 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0072\n",
      "Epoch [1], Batch [10/12], Loss: 0.0090\n",
      "Round 99 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0083\n",
      "Epoch [1], Batch [10/12], Loss: 0.0107\n",
      "Round 100 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0078\n",
      "Epoch [1], Batch [10/12], Loss: 0.0095\n",
      "Round 101 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0080\n",
      "Epoch [1], Batch [10/12], Loss: 0.0104\n",
      "Round 102 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0074\n",
      "Epoch [1], Batch [10/12], Loss: 0.0083\n",
      "Round 103 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0077\n",
      "Epoch [1], Batch [10/12], Loss: 0.0098\n",
      "Round 104 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0060\n",
      "Epoch [1], Batch [10/12], Loss: 0.0083\n",
      "Round 105 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0070\n",
      "Epoch [1], Batch [10/12], Loss: 0.0095\n",
      "Round 106 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0067\n",
      "Epoch [1], Batch [10/12], Loss: 0.0087\n",
      "Round 107 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0073\n",
      "Epoch [1], Batch [10/12], Loss: 0.0078\n",
      "Round 108 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0074\n",
      "Epoch [1], Batch [10/12], Loss: 0.0081\n",
      "Round 109 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0072\n",
      "Epoch [1], Batch [10/12], Loss: 0.0094\n",
      "Round 110 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0070\n",
      "Epoch [1], Batch [10/12], Loss: 0.0097\n",
      "Round 111 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0067\n",
      "Epoch [1], Batch [10/12], Loss: 0.0084\n",
      "Round 112 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0066\n",
      "Epoch [1], Batch [10/12], Loss: 0.0089\n",
      "Round 113 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0066\n",
      "Epoch [1], Batch [10/12], Loss: 0.0093\n",
      "Round 114 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0064\n",
      "Epoch [1], Batch [10/12], Loss: 0.0068\n",
      "Round 115 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0070\n",
      "Epoch [1], Batch [10/12], Loss: 0.0085\n",
      "Round 116 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0060\n",
      "Epoch [1], Batch [10/12], Loss: 0.0086\n",
      "Round 117 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0071\n",
      "Epoch [1], Batch [10/12], Loss: 0.0087\n",
      "Round 118 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0066\n",
      "Epoch [1], Batch [10/12], Loss: 0.0084\n",
      "Round 119 complete\n",
      "Epoch [1], Batch [10/12], Loss: 0.0066\n",
      "Epoch [1], Batch [10/12], Loss: 0.0087\n",
      "Round 120 complete\n"
     ]
    }
   ],
   "source": [
    "def train_local_model(model, train_loader, epochs=1, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0 \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 9:\n",
    "                print(f'Epoch [{epoch+1}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "def federated_averaging(models, train_loaders, global_epochs=10, local_epochs=1, lr=0.01, avg_weight=[0.5, 0.5]):\n",
    "    global_model = SimpleCNN()\n",
    "    \n",
    "    for round in range(global_epochs):\n",
    "        for i, model in enumerate(models):\n",
    "            train_local_model(model, train_loaders[i], epochs=local_epochs, lr=lr)\n",
    "        \n",
    "        global_model = average_model_parameters(models, avg_weight)\n",
    "        \n",
    "        for model in models:\n",
    "            update_model_parameters(model, global_model.parameters())\n",
    "        \n",
    "        print(f'Round {round+1} complete')\n",
    "\n",
    "federated_averaging([model1, model2], [train_loader1, train_loader2], global_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:31:57.743966Z",
     "iopub.status.busy": "2024-10-10T13:31:57.742974Z",
     "iopub.status.idle": "2024-10-10T13:32:08.572732Z",
     "shell.execute_reply": "2024-10-10T13:32:08.571430Z",
     "shell.execute_reply.started": "2024-10-10T13:31:57.743907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 accuracy: 95.37%\n",
      "Model 2 accuracy: 95.37%\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), batch_size=1000, shuffle=False)\n",
    "\n",
    "accuracy1 = test_model(model1, test_loader)\n",
    "accuracy2 = test_model(model2, test_loader)\n",
    "\n",
    "print(f'Model 1 accuracy: {accuracy1 * 100:.2f}%')\n",
    "print(f'Model 2 accuracy: {accuracy2 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
